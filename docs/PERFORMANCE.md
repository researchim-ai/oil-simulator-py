# Производительность и ограничения

## 1. Текущие показатели

| Сценарий | Размер сетки | Уровней AMG | Время построения AMG | Время solve (1 шаг) | Память GPU |
|----------|--------------|-------------|-----------------------|---------------------|------------|
| `medium_2d.json` | 128 × 128 × 1 | 6 | 0.8 с | 0.3 с | < 2 ГБ |
| `impes_large_multiwell_3d.json` | 100 × 100 × 5 | 8 | 18 с | 6–8 с | ~8 ГБ |
| `mega_3phase_million.json` | 100 × 100 × 100 | 15 | 216 с | 233 с (вкл. AMG build) | ~22 ГБ |

- Значения указаны для GPU с 24 ГБ (например, RTX 6000 Ada).
- Для «mega»-кейса повторные шаги с использованием кэша занимают 20–30 с (без перестроения иерархии).

## 2. Главные узкие места

1. **Построение `P`** (`build_prolongation_rs_full`): 110–115 с на L0 при миллионе ячеек. Основные затраты — сортировки и scatter-операции.
2. **RAP (`Pᵀ A P`)**: 10–13 с на L0, 5 с на L1. Ограничение — количество ненулевых элементов и уникализация `(row, col)` внутри блоков.
3. **Память**: `P` первого уровня содержит ≈ 5.3 млн ненулевых элементов в float64, что вместе с буферами занимает до 10 ГБ.
4. **GMRES на глубоких уровнях**: из-за консервативного `grad_rel` иногда требуется 5–6 V-циклов.

## 3. Оптимизации, уже реализованные

- Полная векторизация построения `P` и RAP в PyTorch (без Python-циклов по строкам).
- Блочный `_csr_spmm` без глобального `coalesce` с ограничением по 64 строкам.
- Кэширование иерархии между временными шагами.
- Адаптивное эквилибрирование диагонали матрицы.
- Тонкая настройка near-nullspace и ограничение отрицательных весов.

## 4. Планируемые улучшения

- **Смешанная точность:** хранить `P` и грубые уровни в float32 → экономия памяти ×2, снижение времени RAP.
- **Альтернативные схемы коэрснинга:** PMIS/SA с агрессивным отбором C-точек снизят `nnz(P)` и время построения.
- **Kernel-level оптимизация:** вынести критические участки в CUDA/Triton ядра (сборка весов и RAP).
- **Lazy RAP:** отказаться от явного хранения некоторых уровней, выполняя `A_c x` через последовательность `R(A(P x))`.
- **Много-GPU или CPU+GPU схема:** хранить глубокие уровни на CPU, чтобы освободить память.

## 5. Сравнение с промышленными решателями

- Промышленные симуляторы (Eclipse, Intersect) распределяют расчёт по сотням CPU-ядер или нескольким GPU и используют оптимизированные коэрснеры/предобуславливатели. На одной 24 ГБ GPU миллионная сетка считается как stress-test; 30 млн ячеек требуют кластерного решения.
- Наша реализация стремится к «инженерному» качеству для задач до 1–2 млн ячеек на одной GPU, с постепенным переходом к многогпу при внедрении распределённого AMG.

## 6. Рекомендации по параметрам

- `theta`: 0.18–0.25. Увеличение уменьшает память, но ухудшает `grad_rel`.
- `max_levels`: хватит 15–18 для сеток до 1e6.
- `coarsest_size`: 100–150 (баланс между грубым решением и числом уровней).
- `max_cycles`: 10–15 (обычно хватает 6).
- `tol`: 1e-3 для производственных расчётов; 5e-4 для строгости.

## 7. Мониторинг

- Включите профилировщик PyTorch (`torch.profiler`) на первых шагах, чтобы увидеть распределение времени.
- Логи `ClassicalAMG` выводят `const_rel`, `grad_rel`, `neg_w`. Следите за всплесками — это сигнал к настройке `theta` или near-nullspace.

## 8. Ограничения

- Сильно анизотропные пласты (k_h/k_v > 10³) могут требовать изменения near-nullspace (добавление направлений) и более тщательной нормировки.
- В случае дистантных скважин с разрывными коэффициентами может потребоваться локальное сглаживание матрицы или ограничение капиллярных членов.

